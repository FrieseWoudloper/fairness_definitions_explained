---
title: "Fairness Definitions Explained"
output: html_notebook
---

Be sure that Miniconda and the necessary Python libraries are installed! You can do this by executing `./R/setup.R`.

Load `reticulate` package.

```{r}
library(reticulate)
```

Import python libraries.

```{python}
import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from sklearn.model_selection import RepeatedKFold, KFold
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
import copy
from random import randint
from sklearn.feature_selection import RFE
```

Read dataset.

```{python}
DATA_SET_PATH = "./inputs/german.csv"
dataset = pd.read_csv(DATA_SET_PATH)
dataset.head()
```
Create function.

```{python}
def dataset_headers(dataset):
    return list(dataset.columns.values)
```

Call function.

```{python}
header = dataset_headers(dataset)
print(headers)
```

Preprocess data.

```{python}
dataset.target.replace([1, 2], [1, 0], inplace = True)          # this replaces 1 (good credit) with 1 and 2 (bad credit) with 0
tar = dataset.target                                            # target is separated
dataset.drop('target', axis=1, inplace=True)                    # removes target from it, so now dataset just contains the columns on which it has to be trained
dataset.drop('Personal-status-and-sex', axis=1, inplace = True) 
dataset = pd.get_dummies(dataset, columns = ["Purpose", "Credit-history", "Other-debtors", "Property", "Other-installment-plans", "Housing", "Job", "Present-employment-since"])
headers = dataset_headers(dataset)
dataset.head()
```

Train model.

```{python}
weights = {}

def train_logistic_regression(train_x, train_y):
    logistic_regression_model = LogisticRegression(penalty='l2', C=1.0)
    logistic_regression_model.fit(train_x, train_y)
    return logistic_regression_model

rkf = RepeatedKFold(n_splits = 10, n_repeats = 10, random_state = None)
df = dataset.values          # converts it to a numpy array which is easier to handle
z = tar.values                  # they are changed to numpy matrix in corresponding order, so no problem
sc = StandardScaler()

for train_index, test_index in rkf.split(df):
   train_x, test_x = df[train_index], df[test_index]
   train_y, test_y = z[train_index], z[test_index]
   sc.fit(train_x) # Scaler fitten
   X_train_std = sc.transform(train_x) # Scaler toepassen op training set
   X_test_std = sc.transform(test_x) # Scaler toepassen op test set
   trained_logistic_regression_model = train_logistic_regression(X_train_std, train_y) 
   coefficients = np.transpose(trained_logistic_regression_model.coef_[0])
   for f in range(len(coefficients)):
       if headers[f] in weights:
           weights[headers[f]].append(coefficients[f])
       else:
           weights[headers[f]] = [coefficients[f]]           # create a list here
```

```{python}
weight = []
for i in weights:
   weights[i] = sum(weights[i])*1.0/len(weights[i])
   weight.append((i, weights[i]))
# weight = sorted(weight, key=abs(a[1]), reverse=True)
# for i in weight:
#    print i
# print '\n\n\n'
```

Let's give the {fairness} package a try.

```{r}
library(fairness)
library(dplyr)
data(germancredit)
head(germancredit)
```

Calculate group fairness a.k.a. statistical parity a.k.a. proportional parity.

```{r}
germancredit %>%
  group_by(Female) %>%
  summarise(acceptance_rate = (n() - sum(predicted))/n()) %>%
  mutate(statistical_parity = (acceptance_rate) / last(acceptance_rate))
```
```{r}
data <- mutate(germancredit, outcome = ifelse(predicted == 0, 'Good', 'Bad'))
```

```{r}
   prop_parity(
      data         = data, 
      outcome      = 'predicted',
      group        = 'Female',
      preds        = 'predicted',
      outcome_base = 1,
      base         = 'Male')
```
